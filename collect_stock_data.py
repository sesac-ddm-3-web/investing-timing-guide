#!/usr/bin/env python3
"""
Stock Data Collector
ì£¼ì‹ ë°ì´í„°ë¥¼ Yahoo Financeì—ì„œ ìˆ˜ì§‘í•˜ì—¬ JSONìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
ì„œë²„ ì‹¤í–‰ ì—¬ë¶€ì™€ ë¬´ê´€í•˜ê²Œ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì˜ì¡´ì„±: í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì‚¬ìš© (ì¶”ê°€ ì„¤ì¹˜ ë¶ˆí•„ìš”)
"""

import json
import os
import urllib.request
import urllib.error
from datetime import datetime, timedelta
import time
import sys
import csv
from io import StringIO
import ssl

# ì„¤ì •
TICKERS = ["QQQ", "VOO", "SOXX"]
YEARS_BACK = 2  # 2ë…„ì¹˜ ë°ì´í„°ë©´ ì¶©ë¶„
DATA_DIR = "src/main/resources/data"
DELAY_SECONDS = 15  # Alpha Vantage ë¬´ë£Œ APIëŠ” ë¶„ë‹¹ 5íšŒ ì œí•œ (12ì´ˆ ê°„ê²© ê¶Œì¥)

# Alpha Vantage API Key (ë¬´ë£Œ ë°œê¸‰: https://www.alphavantage.co/support/#api-key)
# í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ê±°ë‚˜ ì—¬ê¸°ì— ì§ì ‘ ì…ë ¥
ALPHA_VANTAGE_API_KEY = os.environ.get('ALPHA_VANTAGE_API_KEY', 'demo')  # 'demo'ëŠ” ì œí•œì 

def ensure_data_dir():
    """ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±"""
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)
        print(f"âœ“ Created directory: {DATA_DIR}")

def fetch_stock_data(ticker, years_back=2):
    """
    Alpha Vantage APIì—ì„œ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘

    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼ (ì˜ˆ: "QQQM")
        years_back: ê³¼ê±° ëª‡ ë…„ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ì§€

    Returns:
        list: ì£¼ì‹ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ë‚ ì§œìˆœ ì •ë ¬)
    """
    try:
        print(f"\nğŸ“Š Fetching data for {ticker}...")

        if ALPHA_VANTAGE_API_KEY == 'demo':
            print(f"âš ï¸  Using demo API key (limited data)")
            print(f"   Get free API key at: https://www.alphavantage.co/support/#api-key")

        # Alpha Vantage API URL - TIME_SERIES_DAILY_ADJUSTED (ìµœëŒ€ 20ë…„ ë°ì´í„°)
        url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&apikey={ALPHA_VANTAGE_API_KEY}&outputsize=full"

        # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        req = urllib.request.Request(url, headers=headers)
        ssl_context = ssl._create_unverified_context()
        response = urllib.request.urlopen(req, timeout=30, context=ssl_context)
        data = json.loads(response.read().decode('utf-8'))

        # API ì—ëŸ¬ ì²´í¬
        if 'Error Message' in data:
            print(f"âŒ API Error: {data['Error Message']}")
            return []

        if 'Note' in data:
            print(f"âŒ API Rate Limit: {data['Note']}")
            print(f"   Alpha Vantage ë¬´ë£Œ APIëŠ” ë¶„ë‹¹ 5íšŒ, ì¼ì¼ 100íšŒ ì œí•œì´ ìˆìŠµë‹ˆë‹¤.")
            return []

        if 'Time Series (Daily)' not in data:
            print(f"âŒ No time series data found for {ticker}")
            return []

        # JSON íŒŒì‹±
        time_series = data['Time Series (Daily)']
        stock_data = []

        # ë‚ ì§œ í•„í„°ë§ (years_backë§Œí¼)
        cutoff_date = datetime.now() - timedelta(days=years_back * 365)

        for date_str, values in time_series.items():
            try:
                date_obj = datetime.strptime(date_str, '%Y-%m-%d')

                # ì§€ì •ëœ ê¸°ê°„ ë‚´ì˜ ë°ì´í„°ë§Œ í¬í•¨
                if date_obj < cutoff_date:
                    continue

                stock_data.append({
                    "date": date_str,
                    "open": float(values['1. open']),
                    "high": float(values['2. high']),
                    "low": float(values['3. low']),
                    "close": float(values['4. close']),
                    "volume": int(values['6. volume'])
                })
            except (ValueError, KeyError) as e:
                # ì˜ëª»ëœ ë°ì´í„°ëŠ” ê±´ë„ˆë›°ê¸°
                continue

        if not stock_data:
            print(f"âš ï¸  No data found for {ticker}")
            return []

        # ë‚ ì§œìˆœ ì •ë ¬
        stock_data.sort(key=lambda x: x['date'])

        print(f"âœ“ Successfully fetched {len(stock_data)} records for {ticker}")
        return stock_data

    except urllib.error.HTTPError as e:
        print(f"âŒ HTTP Error {e.code} for {ticker}: {e.reason}")
        return []
    except Exception as e:
        print(f"âŒ Error fetching data for {ticker}: {str(e)}")
        return []

def save_to_json(ticker, data):
    """
    ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥

    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼
        data: ì €ì¥í•  ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    try:
        file_path = os.path.join(DATA_DIR, f"{ticker}.json")

        with open(file_path, 'w') as f:
            json.dump(data, f, indent=2)

        print(f"âœ“ Saved to {file_path}")

    except Exception as e:
        print(f"âŒ Error saving data for {ticker}: {str(e)}")

def collect_all_data():
    """ëª¨ë“  í‹°ì»¤ì˜ ë°ì´í„° ìˆ˜ì§‘"""
    print("=" * 60)
    print("Stock Data Collector (Alpha Vantage)")
    print("=" * 60)
    print(f"Tickers: {', '.join(TICKERS)}")
    print(f"Period: Last {YEARS_BACK} years")
    print(f"Delay between requests: {DELAY_SECONDS} seconds")
    print(f"API Key: {ALPHA_VANTAGE_API_KEY[:4]}..." if ALPHA_VANTAGE_API_KEY != 'demo' else "API Key: demo (limited)")
    print("=" * 60)

    # ë°ì´í„° ë””ë ‰í† ë¦¬ í™•ì¸/ìƒì„±
    ensure_data_dir()

    success_count = 0
    failed_count = 0

    for i, ticker in enumerate(TICKERS):
        # Rate limiting ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸° (ì²« ë²ˆì§¸ ìš”ì²­ì€ ì œì™¸)
        if i > 0:
            print(f"\nâ³ Waiting {DELAY_SECONDS} seconds to avoid rate limiting...")
            time.sleep(DELAY_SECONDS)

        # ë°ì´í„° ìˆ˜ì§‘
        data = fetch_stock_data(ticker, YEARS_BACK)

        if data:
            save_to_json(ticker, data)
            success_count += 1
        else:
            failed_count += 1

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("Collection Complete!")
    print("=" * 60)
    print(f"âœ“ Success: {success_count} tickers")
    print(f"âŒ Failed: {failed_count} tickers")

    if success_count > 0:
        print(f"\nData saved in: {DATA_DIR}/")
        print("\nYou can now start the Spring Boot application and use the data!")

    return success_count, failed_count

def update_recent_data(ticker, days_back=7):
    """
    ìµœê·¼ ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸ (ì¦ë¶„ ì—…ë°ì´íŠ¸)

    Args:
        ticker: ì£¼ì‹ í‹°ì»¤ ì‹¬ë³¼
        days_back: ìµœê·¼ ë©°ì¹ ì¹˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ì§€
    """
    try:
        print(f"\nğŸ“Š Updating recent data for {ticker} (last {days_back} days)...")

        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        file_path = os.path.join(DATA_DIR, f"{ticker}.json")
        existing_data = []

        if os.path.exists(file_path):
            with open(file_path, 'r') as f:
                existing_data = json.load(f)
            print(f"âœ“ Loaded {len(existing_data)} existing records")

        # ìµœê·¼ ë°ì´í„° ìˆ˜ì§‘
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        period1 = int(start_date.timestamp())
        period2 = int(end_date.timestamp())

        url = f"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={ticker}&apikey={ALPHA_VANTAGE_API_KEY}&outputsize=compact"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        req = urllib.request.Request(url, headers=headers)
        ssl_context = ssl._create_unverified_context()
        response = urllib.request.urlopen(req, timeout=30, context=ssl_context)
        data = json.loads(response.read().decode('utf-8'))

        if 'Time Series (Daily)' not in data:
            print(f"âš ï¸  No recent data found for {ticker}")
            return

        # JSON íŒŒì‹±
        time_series = data['Time Series (Daily)']
        new_records = []

        # ë‚ ì§œ í•„í„°ë§
        cutoff_date = datetime.now() - timedelta(days=days_back)

        for date_str, values in time_series.items():
            try:
                date_obj = datetime.strptime(date_str, '%Y-%m-%d')

                if date_obj < cutoff_date:
                    continue

                new_records.append({
                    "date": date_str,
                    "open": float(values['1. open']),
                    "high": float(values['2. high']),
                    "low": float(values['3. low']),
                    "close": float(values['4. close']),
                    "volume": int(values['6. volume'])
                })
            except (ValueError, KeyError):
                continue

        if not new_records:
            print(f"âš ï¸  No recent data found for {ticker}")
            return

        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© (ì¤‘ë³µ ì œê±°)
        existing_dates = {record['date'] for record in existing_data}
        new_count = 0

        for record in new_records:
            if record['date'] not in existing_dates:
                existing_data.append(record)
                new_count += 1

        # ë‚ ì§œìˆœ ì •ë ¬
        existing_data.sort(key=lambda x: x['date'])

        # ì €ì¥
        with open(file_path, 'w') as f:
            json.dump(existing_data, f, indent=2)

        print(f"âœ“ Added {new_count} new records (total: {len(existing_data)})")

    except Exception as e:
        print(f"âŒ Error updating data for {ticker}: {str(e)}")

def update_all_recent():
    """ëª¨ë“  í‹°ì»¤ì˜ ìµœê·¼ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    print("=" * 60)
    print("Update Recent Data (Last 7 Days)")
    print("=" * 60)

    for i, ticker in enumerate(TICKERS):
        if i > 0:
            print(f"\nâ³ Waiting {DELAY_SECONDS} seconds...")
            time.sleep(DELAY_SECONDS)

        update_recent_data(ticker, days_back=7)

    print("\n" + "=" * 60)
    print("Update Complete!")
    print("=" * 60)

if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "--update":
        # ìµœê·¼ ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
        update_all_recent()
    else:
        # ì „ì²´ ë°ì´í„° ìˆ˜ì§‘
        collect_all_data()
